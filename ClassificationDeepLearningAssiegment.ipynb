{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbdullahAlTalaq/About-Me/blob/main/ClassificationDeepLearningAssiegment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOYcAdUR40ai"
      },
      "source": [
        "# ASSIGNMENT 1: Iris Data Classification (Using TensorFlow)\n",
        "## Prepared by [Mustafa Youldash, Ph.D.](https://github.com/youldash)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykT2ZNIC40aj"
      },
      "source": [
        "### The Iris Data Set (i.e., Problem Set)\n",
        "\n",
        "The [Iris data set](https://archive.ics.uci.edu/ml/datasets/Iris/) is a popular data set for classification tasks in machine learning. It consists of 150 samples of iris plants, with each sample consisting of four features (sepal length, sepal width, petal length, and petal width) and a target label indicating the species of the iris plant (setosa, versicolor, or virginica)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jMWDE6T40aj"
      },
      "source": [
        "To solve the assignment using the Iris data set, students would need to preprocess the data, develop and train a Deep Learning model, and evaluate the performance of the model. Preprocessing the data might involve scaling the features and splitting the data into training and validation sets. Developing and training the model could involve selecting an appropriate architecture and optimization algorithm, setting the learning rate, and choosing the number of epochs. Evaluating the performance of the model could involve using metrics such as accuracy, precision, and recall to assess the model's ability to classify the iris plants correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2--X1Gz240ak",
        "outputId": "4781034c-dee8-4a7c-eb01-fc21acb35f87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]\n"
          ]
        }
      ],
      "source": [
        "# What version of Python do you currently have?\n",
        "import sys\n",
        "\n",
        "\n",
        "print(sys.version)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNkR3pnF40ak"
      },
      "outputs": [],
      "source": [
        "# Do you have TensorFlow installed on your system?\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUXREeKq7gBr",
        "outputId": "f28b0bf9-20af-476b-f53e-15bf414e42ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-utils in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>3.10.0.2 in /usr/local/lib/python3.10/dist-packages (from python-utils) (4.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znRr44ck40al"
      },
      "source": [
        "## Helpful Functions for Keras and TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "py8jVkm240al",
        "outputId": "338ee10f-fc85-4549-9477-08f1c8daeeaf"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-a7bfb22085b7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'util'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from util import helper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBeBNlwN40al"
      },
      "source": [
        "## Exploratory Data Analysis\n",
        "\n",
        "Exploratory Data Analysis (EDA) is a process of analyzing and summarizing a data set in order to understand the underlying structure and relationships within the data. EDA is an important step in the data science process, as it allows you to identify patterns, trends, and anomalies in the data that may not be immediately apparent.\n",
        "\n",
        "There are several benefits of performing EDA for Deep Learning:\n",
        "\n",
        "- EDA helps you understand the data: By performing EDA, you can get a better understanding of the data you are working with, including the distribution of the data, the relationships between different features, and any missing or corrupted values.\n",
        "- EDA can identify potential problems: EDA can help you identify potential problems with the data, such as missing values or outliers, which could impact the performance of your Deep Learning model.\n",
        "- EDA can inform model selection: EDA can help you understand the characteristics of the data, which can inform your choice of Deep Learning model. For example, if the data is highly non-linear, you may want to consider using a model that is capable of capturing complex relationships, such as a neural network.\n",
        "- EDA can improve model performance: By understanding the underlying structure of the data, you can better tune the hyperparameters of your Deep Learning model, which can lead to improved performance.\n",
        "\n",
        "Overall, EDA is an important step in the Deep Learning process, as it helps you understand the data and identify potential issues that could impact the performance of your model. EDA is open-ended, and it is up to you to decide how to look at different ways to slice and dice your data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryFS-0O440al"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "\n",
        "path = \"./data/\"\n",
        "\n",
        "filename = os.path.join(path, \"iris.csv\")\n",
        "df = pd.read_csv(filename, na_values=['NA','?'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTOrUpLz40al"
      },
      "outputs": [],
      "source": [
        "# Hint: use a DataFrame for both EDA and model development."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdTC6ZYC40al"
      },
      "outputs": [],
      "source": [
        "# Your code goes here..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAL8tlaJ40am"
      },
      "source": [
        "# Iris Flower Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZOLuYwj40am"
      },
      "outputs": [],
      "source": [
        "# Imports.\n",
        "import pandas as pd\n",
        "import io\n",
        "import requests\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOYJMJVa40am"
      },
      "outputs": [],
      "source": [
        "# File path.\n",
        "path = \"./data/\"\n",
        "\n",
        "# Read the data.\n",
        "filename = os.path.join(path, \"iris.csv\")\n",
        "df = pd.read_csv(filename, na_values=['NA','?'])\n",
        "\n",
        "# Encode text values to indexes (i.e., [1],[2],[3] for (red,green,blue) values).\n",
        "species = helper.encode_text_index(df, \"species\")\n",
        "\n",
        "# Convert a Pandas DataFrame to the (x,y) inputs that TensorFlow needs.\n",
        "x, y = helper.to_xy(df, \"species\")\n",
        "\n",
        "# Split the data into training and testing sets.\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    x, y, test_size=FIXME, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SQGmJ-t40am"
      },
      "outputs": [],
      "source": [
        "# Define, and build your model.\n",
        "model = Sequential()\n",
        "model.add(Dense(FIXME, input_dim=x.shape[1], kernel_initializer='normal',\n",
        "                activation='FIXME')) # Hint: try different activation functions and see which one produces better results.\n",
        "model.add(Dense(1, kernel_initializer='normal'))\n",
        "model.add(Dense(y.shape[1],activation='FIXME'))\n",
        "\n",
        "# Compile the model.\n",
        "model.compile(\n",
        "    loss='FIXME',\n",
        "    optimizer='FIXME') # Hint: try different optimizers and see which one produces better results.\n",
        "\n",
        "# Define the training callbacks.\n",
        "monitor = EarlyStopping(\n",
        "    monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
        "\n",
        "# Train the model.\n",
        "model.fit(\n",
        "    x, y, validation_data=(x_test,y_test),\n",
        "    callbacks=[monitor], verbose=2, epochs=FIXME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-nlAJTY40am"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "# Evaluate the success rate using accuracy.\n",
        "pred = FIXME\n",
        "\n",
        "y_compare = FIXME\n",
        "\n",
        "# Log the accuracy score.\n",
        "score = metrics.accuracy_score(y_compare, pred)\n",
        "print(\"Accuracy score: {}\".format(score))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6 (v3.10.6:9c7b4bd164, Aug  1 2022, 17:13:48) [Clang 13.0.0 (clang-1300.0.29.30)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}